# Activation Functions

## Topics covered in today's module
* Introduction to Sigmoid, Tanh, ReLU
* Visualizing how ReLU affects the feature maps
* Vanishing and exploding gradients
* Dying ReLU problem
* Advanced activation functions: Swish, GeLU, SeLU

## Main takeaways from doing today's assignment
- Implementation of the activation functions 

## Challenging, interesting, or exciting aspects of today's assignment
- How to visualize what CNN model see the characterisic of the images

## Additional resources used 
- stackoverflow, medium, towardsdatascience
