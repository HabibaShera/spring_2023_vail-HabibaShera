{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MySureStart/spring_2023_vail-HabibaShera/blob/main/Day_05/Introduction_to_Regression_Loss_Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 5 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j",
        "outputId": "6ba92059-8383-4c3c-e79a-c1678aa86213",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57026/57026 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "outputId": "76e87bb0-50c6-450e-d98b-505694cdc103",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "history = model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 7s 43ms/step - loss: 603.5247 - mse: 603.5247 - val_loss: 501.8239 - val_mse: 501.8239\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 582.3229 - mse: 582.3229 - val_loss: 483.4029 - val_mse: 483.4029\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 561.9438 - mse: 561.9438 - val_loss: 461.3965 - val_mse: 461.3965\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 534.9013 - mse: 534.9013 - val_loss: 428.9435 - val_mse: 428.9435\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 493.7434 - mse: 493.7434 - val_loss: 380.8257 - val_mse: 380.8257\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 434.5152 - mse: 434.5152 - val_loss: 311.9894 - val_mse: 311.9894\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 351.5595 - mse: 351.5595 - val_loss: 221.4261 - val_mse: 221.4261\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 249.2001 - mse: 249.2001 - val_loss: 123.3056 - val_mse: 123.3056\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 146.9760 - mse: 146.9760 - val_loss: 55.4962 - val_mse: 55.4962\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 86.4345 - mse: 86.4345 - val_loss: 43.2947 - val_mse: 43.2947\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 70.6448 - mse: 70.6448 - val_loss: 43.8478 - val_mse: 43.8478\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 61.8065 - mse: 61.8065 - val_loss: 34.9567 - val_mse: 34.9567\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 54.0305 - mse: 54.0305 - val_loss: 30.2203 - val_mse: 30.2203\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 48.8245 - mse: 48.8245 - val_loss: 28.9680 - val_mse: 28.9680\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 44.6518 - mse: 44.6518 - val_loss: 27.1828 - val_mse: 27.1828\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 42.0041 - mse: 42.0041 - val_loss: 26.2503 - val_mse: 26.2503\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 39.7669 - mse: 39.7669 - val_loss: 24.5166 - val_mse: 24.5166\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 38.0035 - mse: 38.0035 - val_loss: 25.9714 - val_mse: 25.9714\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 36.9622 - mse: 36.9622 - val_loss: 26.3774 - val_mse: 26.3774\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 35.8573 - mse: 35.8573 - val_loss: 24.7531 - val_mse: 24.7531\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 34.9418 - mse: 34.9418 - val_loss: 25.2348 - val_mse: 25.2348\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 34.2298 - mse: 34.2298 - val_loss: 25.2876 - val_mse: 25.2876\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 33.3846 - mse: 33.3846 - val_loss: 24.7447 - val_mse: 24.7447\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 35ms/step - loss: 32.6708 - mse: 32.6708 - val_loss: 24.7770 - val_mse: 24.7770\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 32.1229 - mse: 32.1229 - val_loss: 25.0853 - val_mse: 25.0853\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 21ms/step - loss: 31.5219 - mse: 31.5219 - val_loss: 23.7501 - val_mse: 23.7501\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 30.8551 - mse: 30.8551 - val_loss: 24.4938 - val_mse: 24.4938\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 30.3064 - mse: 30.3064 - val_loss: 24.1365 - val_mse: 24.1365\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 29.8615 - mse: 29.8615 - val_loss: 23.2172 - val_mse: 23.2172\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 29.3537 - mse: 29.3537 - val_loss: 23.9871 - val_mse: 23.9871\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 28.7902 - mse: 28.7902 - val_loss: 23.1466 - val_mse: 23.1466\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 28.2889 - mse: 28.2889 - val_loss: 22.1010 - val_mse: 22.1010\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 27.7483 - mse: 27.7483 - val_loss: 21.9188 - val_mse: 21.9188\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 27.1318 - mse: 27.1318 - val_loss: 22.8271 - val_mse: 22.8271\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 26.2985 - mse: 26.2985 - val_loss: 21.7297 - val_mse: 21.7297\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 25.8179 - mse: 25.8179 - val_loss: 20.5701 - val_mse: 20.5701\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 25.1858 - mse: 25.1858 - val_loss: 21.1777 - val_mse: 21.1777\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 24.4793 - mse: 24.4793 - val_loss: 21.2142 - val_mse: 21.2142\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 23.9463 - mse: 23.9463 - val_loss: 19.9297 - val_mse: 19.9297\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 23.3223 - mse: 23.3223 - val_loss: 20.7125 - val_mse: 20.7125\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 22.6745 - mse: 22.6745 - val_loss: 20.1354 - val_mse: 20.1354\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 22.1519 - mse: 22.1519 - val_loss: 20.6054 - val_mse: 20.6054\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 21.5576 - mse: 21.5576 - val_loss: 20.1881 - val_mse: 20.1881\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 20.9007 - mse: 20.9007 - val_loss: 18.9507 - val_mse: 18.9507\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 20.4716 - mse: 20.4716 - val_loss: 18.9797 - val_mse: 18.9797\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 19.9685 - mse: 19.9685 - val_loss: 18.8921 - val_mse: 18.8921\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 19.4489 - mse: 19.4489 - val_loss: 18.8060 - val_mse: 18.8060\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 19.0056 - mse: 19.0056 - val_loss: 18.9112 - val_mse: 18.9112\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 18.5886 - mse: 18.5886 - val_loss: 17.8142 - val_mse: 17.8142\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 18.1463 - mse: 18.1463 - val_loss: 18.9086 - val_mse: 18.9086\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 17.7682 - mse: 17.7682 - val_loss: 17.8340 - val_mse: 17.8340\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 17.4564 - mse: 17.4564 - val_loss: 17.0607 - val_mse: 17.0607\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 17.0801 - mse: 17.0801 - val_loss: 18.1714 - val_mse: 18.1714\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 16.9276 - mse: 16.9276 - val_loss: 16.6722 - val_mse: 16.6722\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 16.4370 - mse: 16.4370 - val_loss: 17.0230 - val_mse: 17.0230\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 16.1584 - mse: 16.1584 - val_loss: 16.1658 - val_mse: 16.1658\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 15.8046 - mse: 15.8046 - val_loss: 16.5674 - val_mse: 16.5674\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 15.5286 - mse: 15.5286 - val_loss: 16.5315 - val_mse: 16.5315\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 15.2928 - mse: 15.2928 - val_loss: 15.8797 - val_mse: 15.8797\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 15.0900 - mse: 15.0900 - val_loss: 16.3203 - val_mse: 16.3203\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 14.8332 - mse: 14.8332 - val_loss: 15.5298 - val_mse: 15.5298\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 14.6153 - mse: 14.6153 - val_loss: 16.1050 - val_mse: 16.1050\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 14.4088 - mse: 14.4088 - val_loss: 15.3563 - val_mse: 15.3563\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 14.1869 - mse: 14.1869 - val_loss: 14.9476 - val_mse: 14.9476\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 14.0061 - mse: 14.0061 - val_loss: 15.4178 - val_mse: 15.4178\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 13.7995 - mse: 13.7995 - val_loss: 15.1565 - val_mse: 15.1565\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 13.6093 - mse: 13.6093 - val_loss: 14.6206 - val_mse: 14.6206\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 17ms/step - loss: 13.5312 - mse: 13.5312 - val_loss: 14.3882 - val_mse: 14.3882\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 13.3520 - mse: 13.3520 - val_loss: 15.2611 - val_mse: 15.2611\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 13.1904 - mse: 13.1904 - val_loss: 14.6524 - val_mse: 14.6524\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 13.0441 - mse: 13.0441 - val_loss: 14.3697 - val_mse: 14.3697\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 12.8310 - mse: 12.8310 - val_loss: 13.9739 - val_mse: 13.9739\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.6796 - mse: 12.6796 - val_loss: 14.1492 - val_mse: 14.1492\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.6576 - mse: 12.6576 - val_loss: 14.0108 - val_mse: 14.0108\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 12.4420 - mse: 12.4420 - val_loss: 14.1817 - val_mse: 14.1817\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 12.3102 - mse: 12.3102 - val_loss: 13.7646 - val_mse: 13.7646\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.3004 - mse: 12.3004 - val_loss: 13.5014 - val_mse: 13.5014\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 12.1664 - mse: 12.1664 - val_loss: 14.1962 - val_mse: 14.1962\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.9824 - mse: 11.9824 - val_loss: 13.7350 - val_mse: 13.7350\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.9615 - mse: 11.9615 - val_loss: 13.4684 - val_mse: 13.4684\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 11.7758 - mse: 11.7758 - val_loss: 13.8400 - val_mse: 13.8400\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 11.7550 - mse: 11.7550 - val_loss: 13.3294 - val_mse: 13.3294\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 11.6174 - mse: 11.6174 - val_loss: 13.6519 - val_mse: 13.6519\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 23ms/step - loss: 11.4606 - mse: 11.4606 - val_loss: 13.2738 - val_mse: 13.2738\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 11.4307 - mse: 11.4307 - val_loss: 13.3252 - val_mse: 13.3252\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 11.3264 - mse: 11.3264 - val_loss: 13.1133 - val_mse: 13.1133\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 11.2749 - mse: 11.2749 - val_loss: 13.3015 - val_mse: 13.3015\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 11.0922 - mse: 11.0922 - val_loss: 12.9748 - val_mse: 12.9748\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 10.9994 - mse: 10.9994 - val_loss: 13.1431 - val_mse: 13.1431\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 10.9594 - mse: 10.9594 - val_loss: 13.0125 - val_mse: 13.0125\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 10.8745 - mse: 10.8745 - val_loss: 12.9554 - val_mse: 12.9554\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 10.8301 - mse: 10.8301 - val_loss: 12.7607 - val_mse: 12.7607\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 10.7581 - mse: 10.7581 - val_loss: 12.6704 - val_mse: 12.6704\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.7556 - mse: 10.7556 - val_loss: 12.7841 - val_mse: 12.7841\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 10.5832 - mse: 10.5832 - val_loss: 12.5306 - val_mse: 12.5306\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10.4882 - mse: 10.4882 - val_loss: 12.6821 - val_mse: 12.6821\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10.4400 - mse: 10.4400 - val_loss: 12.6148 - val_mse: 12.6148\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 10.3883 - mse: 10.3883 - val_loss: 12.2446 - val_mse: 12.2446\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10.3551 - mse: 10.3551 - val_loss: 12.4473 - val_mse: 12.4473\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 10.3147 - mse: 10.3147 - val_loss: 12.5859 - val_mse: 12.5859\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 10.1993 - mse: 10.1993 - val_loss: 11.9806 - val_mse: 11.9806\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 10.1789 - mse: 10.1789 - val_loss: 12.6772 - val_mse: 12.6772\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10.0834 - mse: 10.0834 - val_loss: 12.1852 - val_mse: 12.1852\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 10.0050 - mse: 10.0050 - val_loss: 12.1576 - val_mse: 12.1576\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 10.0129 - mse: 10.0129 - val_loss: 12.3244 - val_mse: 12.3244\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.8823 - mse: 9.8823 - val_loss: 12.3845 - val_mse: 12.3845\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.8609 - mse: 9.8609 - val_loss: 11.9954 - val_mse: 11.9954\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.8230 - mse: 9.8230 - val_loss: 12.1473 - val_mse: 12.1473\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.7296 - mse: 9.7296 - val_loss: 11.8556 - val_mse: 11.8556\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 9.6842 - mse: 9.6842 - val_loss: 12.1032 - val_mse: 12.1032\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.6492 - mse: 9.6492 - val_loss: 11.8255 - val_mse: 11.8255\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 9.5830 - mse: 9.5830 - val_loss: 11.5164 - val_mse: 11.5164\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.5570 - mse: 9.5570 - val_loss: 11.8496 - val_mse: 11.8496\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.5631 - mse: 9.5631 - val_loss: 11.7482 - val_mse: 11.7482\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 9.4406 - mse: 9.4406 - val_loss: 11.4915 - val_mse: 11.4915\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4009 - mse: 9.4009 - val_loss: 11.8394 - val_mse: 11.8394\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.6453 - mse: 9.6453 - val_loss: 11.9330 - val_mse: 11.9330\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3922 - mse: 9.3922 - val_loss: 11.1771 - val_mse: 11.1771\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4254 - mse: 9.4254 - val_loss: 11.6109 - val_mse: 11.6109\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.3356 - mse: 9.3356 - val_loss: 11.5553 - val_mse: 11.5553\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.2179 - mse: 9.2179 - val_loss: 11.7049 - val_mse: 11.7049\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1776 - mse: 9.1776 - val_loss: 11.5473 - val_mse: 11.5473\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1562 - mse: 9.1562 - val_loss: 11.5341 - val_mse: 11.5341\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.1470 - mse: 9.1470 - val_loss: 11.4806 - val_mse: 11.4806\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.2070 - mse: 9.2070 - val_loss: 11.6109 - val_mse: 11.6109\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 9.0115 - mse: 9.0115 - val_loss: 11.3705 - val_mse: 11.3705\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.0375 - mse: 9.0375 - val_loss: 11.4754 - val_mse: 11.4754\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9524 - mse: 8.9524 - val_loss: 11.4407 - val_mse: 11.4407\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.8983 - mse: 8.8983 - val_loss: 11.2329 - val_mse: 11.2329\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.9383 - mse: 8.9383 - val_loss: 11.4173 - val_mse: 11.4173\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.9210 - mse: 8.9210 - val_loss: 11.2585 - val_mse: 11.2585\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.7627 - mse: 8.7627 - val_loss: 11.2863 - val_mse: 11.2863\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.8344 - mse: 8.8344 - val_loss: 11.1896 - val_mse: 11.1896\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.7429 - mse: 8.7429 - val_loss: 11.2132 - val_mse: 11.2132\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.7486 - mse: 8.7486 - val_loss: 10.8867 - val_mse: 10.8867\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6951 - mse: 8.6951 - val_loss: 11.0480 - val_mse: 11.0480\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.6400 - mse: 8.6400 - val_loss: 10.9195 - val_mse: 10.9195\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.7527 - mse: 8.7527 - val_loss: 10.8590 - val_mse: 10.8590\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.7757 - mse: 8.7757 - val_loss: 10.7720 - val_mse: 10.7720\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.6247 - mse: 8.6247 - val_loss: 10.5613 - val_mse: 10.5613\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5408 - mse: 8.5408 - val_loss: 10.6126 - val_mse: 10.6126\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5631 - mse: 8.5631 - val_loss: 10.6971 - val_mse: 10.6971\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5152 - mse: 8.5152 - val_loss: 10.5887 - val_mse: 10.5887\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.4887 - mse: 8.4887 - val_loss: 10.6344 - val_mse: 10.6344\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.5733 - mse: 8.5733 - val_loss: 10.9650 - val_mse: 10.9650\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4098 - mse: 8.4098 - val_loss: 10.3627 - val_mse: 10.3627\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.4752 - mse: 8.4752 - val_loss: 10.7008 - val_mse: 10.7008\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.3994 - mse: 8.3994 - val_loss: 10.4004 - val_mse: 10.4004\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.3048 - mse: 8.3048 - val_loss: 10.3589 - val_mse: 10.3589\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.3433 - mse: 8.3433 - val_loss: 10.4451 - val_mse: 10.4451\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.3914 - mse: 8.3914 - val_loss: 10.3886 - val_mse: 10.3886\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.2466 - mse: 8.2466 - val_loss: 10.4167 - val_mse: 10.4167\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.4362 - mse: 8.4362 - val_loss: 10.2119 - val_mse: 10.2119\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.3904 - mse: 8.3904 - val_loss: 10.1201 - val_mse: 10.1201\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.2491 - mse: 8.2491 - val_loss: 10.3514 - val_mse: 10.3514\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2233 - mse: 8.2233 - val_loss: 10.0636 - val_mse: 10.0636\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8.3361 - mse: 8.3361 - val_loss: 10.2685 - val_mse: 10.2685\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 8.2223 - mse: 8.2223 - val_loss: 10.1912 - val_mse: 10.1912\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 8.2947 - mse: 8.2947 - val_loss: 10.2206 - val_mse: 10.2206\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 8.3092 - mse: 8.3092 - val_loss: 10.2653 - val_mse: 10.2653\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 8.3179 - mse: 8.3179 - val_loss: 10.0751 - val_mse: 10.0751\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0795 - mse: 8.0795 - val_loss: 10.3149 - val_mse: 10.3149\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0297 - mse: 8.0297 - val_loss: 10.1183 - val_mse: 10.1183\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.1057 - mse: 8.1057 - val_loss: 10.1079 - val_mse: 10.1079\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0040 - mse: 8.0040 - val_loss: 9.9351 - val_mse: 9.9351\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0678 - mse: 8.0678 - val_loss: 10.2347 - val_mse: 10.2347\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 8.0546 - mse: 8.0546 - val_loss: 10.0485 - val_mse: 10.0485\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0704 - mse: 8.0704 - val_loss: 10.1364 - val_mse: 10.1364\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.1654 - mse: 8.1654 - val_loss: 10.2144 - val_mse: 10.2144\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.9411 - mse: 7.9411 - val_loss: 10.0219 - val_mse: 10.0219\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 8.0015 - mse: 8.0015 - val_loss: 10.2202 - val_mse: 10.2202\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.9789 - mse: 7.9789 - val_loss: 10.0713 - val_mse: 10.0713\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9417 - mse: 7.9417 - val_loss: 9.9583 - val_mse: 9.9583\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0383 - mse: 8.0383 - val_loss: 9.9035 - val_mse: 9.9035\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 7.8431 - mse: 7.8431 - val_loss: 9.9619 - val_mse: 9.9619\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 7.7905 - mse: 7.7905 - val_loss: 9.9813 - val_mse: 9.9813\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7.8220 - mse: 7.8220 - val_loss: 9.6790 - val_mse: 9.6790\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 7.7998 - mse: 7.7998 - val_loss: 9.7653 - val_mse: 9.7653\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7.7128 - mse: 7.7128 - val_loss: 9.9535 - val_mse: 9.9535\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.7697 - mse: 7.7697 - val_loss: 10.0149 - val_mse: 10.0149\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6947 - mse: 7.6947 - val_loss: 10.2599 - val_mse: 10.2599\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.6910 - mse: 7.6910 - val_loss: 10.1566 - val_mse: 10.1566\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.7453 - mse: 7.7453 - val_loss: 9.9716 - val_mse: 9.9716\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.6481 - mse: 7.6481 - val_loss: 9.9601 - val_mse: 9.9601\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.6991 - mse: 7.6991 - val_loss: 9.9871 - val_mse: 9.9871\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.5616 - mse: 7.5616 - val_loss: 9.6868 - val_mse: 9.6868\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.5494 - mse: 7.5494 - val_loss: 9.6044 - val_mse: 9.6044\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.5109 - mse: 7.5109 - val_loss: 9.4995 - val_mse: 9.4995\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.5133 - mse: 7.5133 - val_loss: 9.4778 - val_mse: 9.4778\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.4817 - mse: 7.4817 - val_loss: 9.5600 - val_mse: 9.5600\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.5214 - mse: 7.5214 - val_loss: 9.2876 - val_mse: 9.2876\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.4168 - mse: 7.4168 - val_loss: 9.1667 - val_mse: 9.1667\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.5134 - mse: 7.5134 - val_loss: 9.1411 - val_mse: 9.1411\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4206 - mse: 7.4206 - val_loss: 9.2657 - val_mse: 9.2657\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 7.3298 - mse: 7.3298 - val_loss: 9.1999 - val_mse: 9.1999\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 7.3357 - mse: 7.3357 - val_loss: 9.0342 - val_mse: 9.0342\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 7.3486 - mse: 7.3486 - val_loss: 9.1759 - val_mse: 9.1759\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 7.3372 - mse: 7.3372 - val_loss: 9.1188 - val_mse: 9.1188\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 13ms/step - loss: 7.3481 - mse: 7.3481 - val_loss: 9.1494 - val_mse: 9.1494\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2802 - mse: 7.2802 - val_loss: 8.9625 - val_mse: 8.9625\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2841 - mse: 7.2841 - val_loss: 9.1624 - val_mse: 9.1624\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.2380 - mse: 7.2380 - val_loss: 9.1345 - val_mse: 9.1345\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.2024 - mse: 7.2024 - val_loss: 9.1422 - val_mse: 9.1422\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2184 - mse: 7.2184 - val_loss: 9.0879 - val_mse: 9.0879\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.2051 - mse: 7.2051 - val_loss: 9.2106 - val_mse: 9.2106\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1967 - mse: 7.1967 - val_loss: 9.1801 - val_mse: 9.1801\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1985 - mse: 7.1985 - val_loss: 8.9550 - val_mse: 8.9550\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1653 - mse: 7.1653 - val_loss: 8.8889 - val_mse: 8.8889\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0968 - mse: 7.0968 - val_loss: 8.9752 - val_mse: 8.9752\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1293 - mse: 7.1293 - val_loss: 8.9619 - val_mse: 8.9619\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0466 - mse: 7.0466 - val_loss: 8.9507 - val_mse: 8.9507\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0835 - mse: 7.0835 - val_loss: 8.9196 - val_mse: 8.9196\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0314 - mse: 7.0314 - val_loss: 8.9992 - val_mse: 8.9992\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 7.1112 - mse: 7.1112 - val_loss: 8.9504 - val_mse: 8.9504\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0953 - mse: 7.0953 - val_loss: 9.1509 - val_mse: 9.1509\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0351 - mse: 7.0351 - val_loss: 9.0120 - val_mse: 9.0120\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9509 - mse: 6.9509 - val_loss: 8.8869 - val_mse: 8.8869\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9742 - mse: 6.9742 - val_loss: 8.8062 - val_mse: 8.8062\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9434 - mse: 6.9434 - val_loss: 8.7334 - val_mse: 8.7334\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9463 - mse: 6.9463 - val_loss: 8.6699 - val_mse: 8.6699\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0312 - mse: 7.0312 - val_loss: 8.8442 - val_mse: 8.8442\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.9729 - mse: 6.9729 - val_loss: 8.6264 - val_mse: 8.6264\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.9557 - mse: 6.9557 - val_loss: 8.6607 - val_mse: 8.6607\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.8731 - mse: 6.8731 - val_loss: 8.5042 - val_mse: 8.5042\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8378 - mse: 6.8378 - val_loss: 8.5418 - val_mse: 8.5418\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.8147 - mse: 6.8147 - val_loss: 8.4037 - val_mse: 8.4037\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8999 - mse: 6.8999 - val_loss: 8.4347 - val_mse: 8.4347\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 6.9056 - mse: 6.9056 - val_loss: 8.4391 - val_mse: 8.4391\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.9933 - mse: 6.9933 - val_loss: 8.5349 - val_mse: 8.5349\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.7573 - mse: 6.7573 - val_loss: 8.3311 - val_mse: 8.3311\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.7799 - mse: 6.7799 - val_loss: 8.4953 - val_mse: 8.4953\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.7257 - mse: 6.7257 - val_loss: 8.2568 - val_mse: 8.2568\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.8509 - mse: 6.8509 - val_loss: 8.3734 - val_mse: 8.3734\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.7578 - mse: 6.7578 - val_loss: 8.2799 - val_mse: 8.2799\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.7307 - mse: 6.7307 - val_loss: 8.1744 - val_mse: 8.1744\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 19ms/step - loss: 7.0090 - mse: 7.0090 - val_loss: 8.1097 - val_mse: 8.1097\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 6.7268 - mse: 6.7268 - val_loss: 8.1020 - val_mse: 8.1020\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 30ms/step - loss: 6.6560 - mse: 6.6560 - val_loss: 8.0882 - val_mse: 8.0882\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 32ms/step - loss: 6.6446 - mse: 6.6446 - val_loss: 8.1204 - val_mse: 8.1204\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 18ms/step - loss: 6.6003 - mse: 6.6003 - val_loss: 7.9920 - val_mse: 7.9920\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 6.5816 - mse: 6.5816 - val_loss: 8.0963 - val_mse: 8.0963\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 6.6299 - mse: 6.6299 - val_loss: 8.0919 - val_mse: 8.0919\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.5619 - mse: 6.5619 - val_loss: 8.2667 - val_mse: 8.2667\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.5173 - mse: 6.5173 - val_loss: 8.0440 - val_mse: 8.0440\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.5169 - mse: 6.5169 - val_loss: 8.0579 - val_mse: 8.0579\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5247 - mse: 6.5247 - val_loss: 8.1357 - val_mse: 8.1357\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5033 - mse: 6.5033 - val_loss: 8.0305 - val_mse: 8.0305\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 6.4714 - mse: 6.4714 - val_loss: 8.0042 - val_mse: 8.0042\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 6.4504 - mse: 6.4504 - val_loss: 8.0022 - val_mse: 8.0022\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 6.4674 - mse: 6.4674 - val_loss: 8.1775 - val_mse: 8.1775\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history.history.keys()"
      ],
      "metadata": {
        "id": "ggLlLaI68IXT",
        "outputId": "30617382-b8d9-4554-f31b-a986beecf9a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['loss', 'mse', 'val_loss', 'val_mse'])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RF_LUDbdo2",
        "outputId": "4e6752fa-4af7-44ee-94b8-d8ba394949ba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "errors = np.arange(1,251)\n",
        "\n",
        "mse = history.history['val_mse']\n",
        "\n",
        "plt.plot(errors, mse, c='#ED4F46', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn",
        "outputId": "4ab59e86-4c5f-4f80-e702-0186ac55a6e5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "history = model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 2s 19ms/step - loss: 22.0255 - mae: 22.0255 - val_loss: 20.4545 - val_mae: 20.4545\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 20.7692 - mae: 20.7692 - val_loss: 18.6962 - val_mae: 18.6962\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 18.3938 - mae: 18.3938 - val_loss: 15.2286 - val_mae: 15.2286\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 13.8558 - mae: 13.8558 - val_loss: 9.1176 - val_mae: 9.1176\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 7.6936 - mae: 7.6936 - val_loss: 5.7243 - val_mae: 5.7243\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 5.8974 - mae: 5.8974 - val_loss: 4.9169 - val_mae: 4.9169\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4.4185 - mae: 4.4185 - val_loss: 3.5965 - val_mae: 3.5965\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 3.7292 - mae: 3.7292 - val_loss: 3.6730 - val_mae: 3.6730\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.4433 - mae: 3.4433 - val_loss: 3.1753 - val_mae: 3.1753\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 3.2236 - mae: 3.2236 - val_loss: 3.1517 - val_mae: 3.1517\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 3.0603 - mae: 3.0603 - val_loss: 2.9733 - val_mae: 2.9733\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.8996 - mae: 2.8996 - val_loss: 2.9794 - val_mae: 2.9794\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.8313 - mae: 2.8313 - val_loss: 2.7375 - val_mae: 2.7375\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.6629 - mae: 2.6629 - val_loss: 2.6914 - val_mae: 2.6914\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.5971 - mae: 2.5971 - val_loss: 2.7025 - val_mae: 2.7025\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.4757 - mae: 2.4757 - val_loss: 2.5609 - val_mae: 2.5609\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.3999 - mae: 2.3999 - val_loss: 2.5081 - val_mae: 2.5081\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.3393 - mae: 2.3393 - val_loss: 2.5613 - val_mae: 2.5613\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.3087 - mae: 2.3087 - val_loss: 2.4573 - val_mae: 2.4573\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 2.3584 - mae: 2.3584 - val_loss: 2.4385 - val_mae: 2.4385\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 2.3505 - mae: 2.3505 - val_loss: 2.5572 - val_mae: 2.5572\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 2.2142 - mae: 2.2142 - val_loss: 2.3444 - val_mae: 2.3444\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.1770 - mae: 2.1770 - val_loss: 2.4999 - val_mae: 2.4999\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.1590 - mae: 2.1590 - val_loss: 2.3463 - val_mae: 2.3463\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.1617 - mae: 2.1617 - val_loss: 2.4722 - val_mae: 2.4722\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1344 - mae: 2.1344 - val_loss: 2.3277 - val_mae: 2.3277\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.1222 - mae: 2.1222 - val_loss: 2.2936 - val_mae: 2.2936\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1151 - mae: 2.1151 - val_loss: 2.4027 - val_mae: 2.4027\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.1009 - mae: 2.1009 - val_loss: 2.2900 - val_mae: 2.2900\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0628 - mae: 2.0628 - val_loss: 2.2533 - val_mae: 2.2533\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0582 - mae: 2.0582 - val_loss: 2.3771 - val_mae: 2.3771\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0633 - mae: 2.0633 - val_loss: 2.3656 - val_mae: 2.3656\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0441 - mae: 2.0441 - val_loss: 2.2337 - val_mae: 2.2337\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0317 - mae: 2.0317 - val_loss: 2.3441 - val_mae: 2.3441\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0140 - mae: 2.0140 - val_loss: 2.2258 - val_mae: 2.2258\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.9748 - mae: 1.9748 - val_loss: 2.2567 - val_mae: 2.2567\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9761 - mae: 1.9761 - val_loss: 2.2649 - val_mae: 2.2649\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.9761 - mae: 1.9761 - val_loss: 2.3245 - val_mae: 2.3245\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9453 - mae: 1.9453 - val_loss: 2.2321 - val_mae: 2.2321\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9911 - mae: 1.9911 - val_loss: 2.2648 - val_mae: 2.2648\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9167 - mae: 1.9167 - val_loss: 2.2105 - val_mae: 2.2105\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9184 - mae: 1.9184 - val_loss: 2.3073 - val_mae: 2.3073\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9366 - mae: 1.9366 - val_loss: 2.2244 - val_mae: 2.2244\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8903 - mae: 1.8903 - val_loss: 2.3302 - val_mae: 2.3302\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9313 - mae: 1.9313 - val_loss: 2.2693 - val_mae: 2.2693\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8892 - mae: 1.8892 - val_loss: 2.2486 - val_mae: 2.2486\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9053 - mae: 1.9053 - val_loss: 2.2796 - val_mae: 2.2796\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8810 - mae: 1.8810 - val_loss: 2.2341 - val_mae: 2.2341\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8639 - mae: 1.8639 - val_loss: 2.2575 - val_mae: 2.2575\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8361 - mae: 1.8361 - val_loss: 2.3385 - val_mae: 2.3385\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.9232 - mae: 1.9232 - val_loss: 2.2480 - val_mae: 2.2480\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8868 - mae: 1.8868 - val_loss: 2.2525 - val_mae: 2.2525\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8172 - mae: 1.8172 - val_loss: 2.2306 - val_mae: 2.2306\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8249 - mae: 1.8249 - val_loss: 2.2184 - val_mae: 2.2184\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7783 - mae: 1.7783 - val_loss: 2.2304 - val_mae: 2.2304\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7309 - mae: 1.7309 - val_loss: 2.2586 - val_mae: 2.2586\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7489 - mae: 1.7489 - val_loss: 2.2244 - val_mae: 2.2244\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7560 - mae: 1.7560 - val_loss: 2.1930 - val_mae: 2.1930\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7532 - mae: 1.7532 - val_loss: 2.2120 - val_mae: 2.2120\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7077 - mae: 1.7077 - val_loss: 2.1862 - val_mae: 2.1862\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7067 - mae: 1.7067 - val_loss: 2.1881 - val_mae: 2.1881\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7092 - mae: 1.7092 - val_loss: 2.2003 - val_mae: 2.2003\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6834 - mae: 1.6834 - val_loss: 2.1923 - val_mae: 2.1923\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6835 - mae: 1.6835 - val_loss: 2.0906 - val_mae: 2.0906\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7244 - mae: 1.7244 - val_loss: 2.3061 - val_mae: 2.3061\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.7476 - mae: 1.7476 - val_loss: 2.1636 - val_mae: 2.1636\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7126 - mae: 1.7126 - val_loss: 2.2913 - val_mae: 2.2913\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6839 - mae: 1.6839 - val_loss: 2.1398 - val_mae: 2.1398\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6555 - mae: 1.6555 - val_loss: 2.2051 - val_mae: 2.2051\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6515 - mae: 1.6515 - val_loss: 2.1450 - val_mae: 2.1450\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6448 - mae: 1.6448 - val_loss: 2.1349 - val_mae: 2.1349\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6580 - mae: 1.6580 - val_loss: 2.2186 - val_mae: 2.2186\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6412 - mae: 1.6412 - val_loss: 2.2463 - val_mae: 2.2463\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6833 - mae: 1.6833 - val_loss: 2.0867 - val_mae: 2.0867\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6280 - mae: 1.6280 - val_loss: 2.2651 - val_mae: 2.2651\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6170 - mae: 1.6170 - val_loss: 2.1750 - val_mae: 2.1750\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5954 - mae: 1.5954 - val_loss: 2.1070 - val_mae: 2.1070\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5586 - mae: 1.5586 - val_loss: 2.2196 - val_mae: 2.2196\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5499 - mae: 1.5499 - val_loss: 2.2167 - val_mae: 2.2167\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5775 - mae: 1.5775 - val_loss: 2.1680 - val_mae: 2.1680\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5955 - mae: 1.5955 - val_loss: 2.2850 - val_mae: 2.2850\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5664 - mae: 1.5664 - val_loss: 2.1249 - val_mae: 2.1249\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5812 - mae: 1.5812 - val_loss: 2.2197 - val_mae: 2.2197\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5986 - mae: 1.5986 - val_loss: 2.1896 - val_mae: 2.1896\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5174 - mae: 1.5174 - val_loss: 2.1752 - val_mae: 2.1752\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.5529 - mae: 1.5529 - val_loss: 2.2355 - val_mae: 2.2355\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5338 - mae: 1.5338 - val_loss: 2.2304 - val_mae: 2.2304\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5627 - mae: 1.5627 - val_loss: 2.1261 - val_mae: 2.1261\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5596 - mae: 1.5596 - val_loss: 2.2075 - val_mae: 2.2075\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5392 - mae: 1.5392 - val_loss: 2.2175 - val_mae: 2.2175\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5194 - mae: 1.5194 - val_loss: 2.1845 - val_mae: 2.1845\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4924 - mae: 1.4924 - val_loss: 2.2051 - val_mae: 2.2051\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4956 - mae: 1.4956 - val_loss: 2.1716 - val_mae: 2.1716\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5184 - mae: 1.5184 - val_loss: 2.1625 - val_mae: 2.1625\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4591 - mae: 1.4591 - val_loss: 2.2175 - val_mae: 2.2175\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4606 - mae: 1.4606 - val_loss: 2.2229 - val_mae: 2.2229\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4676 - mae: 1.4676 - val_loss: 2.1977 - val_mae: 2.1977\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.4986 - mae: 1.4986 - val_loss: 2.1970 - val_mae: 2.1970\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.4351 - mae: 1.4351 - val_loss: 2.2010 - val_mae: 2.2010\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.5059 - mae: 1.5059 - val_loss: 2.1263 - val_mae: 2.1263\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.4381 - mae: 1.4381 - val_loss: 2.1660 - val_mae: 2.1660\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 15ms/step - loss: 1.4001 - mae: 1.4001 - val_loss: 2.1952 - val_mae: 2.1952\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 20ms/step - loss: 1.3882 - mae: 1.3882 - val_loss: 2.1730 - val_mae: 2.1730\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.3701 - mae: 1.3701 - val_loss: 2.2282 - val_mae: 2.2282\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 1.3943 - mae: 1.3943 - val_loss: 2.2038 - val_mae: 2.2038\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.4086 - mae: 1.4086 - val_loss: 2.1926 - val_mae: 2.1926\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4847 - mae: 1.4847 - val_loss: 2.2085 - val_mae: 2.2085\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4241 - mae: 1.4241 - val_loss: 2.1438 - val_mae: 2.1438\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4444 - mae: 1.4444 - val_loss: 2.1991 - val_mae: 2.1991\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4343 - mae: 1.4343 - val_loss: 2.1577 - val_mae: 2.1577\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3666 - mae: 1.3666 - val_loss: 2.2099 - val_mae: 2.2099\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3559 - mae: 1.3559 - val_loss: 2.2152 - val_mae: 2.2152\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3996 - mae: 1.3996 - val_loss: 2.2140 - val_mae: 2.2140\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4108 - mae: 1.4108 - val_loss: 2.2463 - val_mae: 2.2463\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3358 - mae: 1.3358 - val_loss: 2.2255 - val_mae: 2.2255\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3228 - mae: 1.3228 - val_loss: 2.1979 - val_mae: 2.1979\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3240 - mae: 1.3240 - val_loss: 2.1626 - val_mae: 2.1626\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3140 - mae: 1.3140 - val_loss: 2.1613 - val_mae: 2.1613\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3453 - mae: 1.3453 - val_loss: 2.1815 - val_mae: 2.1815\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3268 - mae: 1.3268 - val_loss: 2.1619 - val_mae: 2.1619\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3250 - mae: 1.3250 - val_loss: 2.2014 - val_mae: 2.2014\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3101 - mae: 1.3101 - val_loss: 2.2470 - val_mae: 2.2470\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4938 - mae: 1.4938 - val_loss: 2.2349 - val_mae: 2.2349\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3972 - mae: 1.3972 - val_loss: 2.2418 - val_mae: 2.2418\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3582 - mae: 1.3582 - val_loss: 2.1638 - val_mae: 2.1638\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2965 - mae: 1.2965 - val_loss: 2.1445 - val_mae: 2.1445\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3108 - mae: 1.3108 - val_loss: 2.1473 - val_mae: 2.1473\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2775 - mae: 1.2775 - val_loss: 2.1487 - val_mae: 2.1487\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2702 - mae: 1.2702 - val_loss: 2.1324 - val_mae: 2.1324\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2989 - mae: 1.2989 - val_loss: 2.1993 - val_mae: 2.1993\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2759 - mae: 1.2759 - val_loss: 2.1099 - val_mae: 2.1099\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2436 - mae: 1.2436 - val_loss: 2.1499 - val_mae: 2.1499\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2801 - mae: 1.2801 - val_loss: 2.1766 - val_mae: 2.1766\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2983 - mae: 1.2983 - val_loss: 2.0977 - val_mae: 2.0977\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2518 - mae: 1.2518 - val_loss: 2.1379 - val_mae: 2.1379\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2504 - mae: 1.2504 - val_loss: 2.1142 - val_mae: 2.1142\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2489 - mae: 1.2489 - val_loss: 2.1080 - val_mae: 2.1080\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2405 - mae: 1.2405 - val_loss: 2.1495 - val_mae: 2.1495\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2654 - mae: 1.2654 - val_loss: 2.1194 - val_mae: 2.1194\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2996 - mae: 1.2996 - val_loss: 2.1336 - val_mae: 2.1336\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2119 - mae: 1.2119 - val_loss: 2.1066 - val_mae: 2.1066\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2701 - mae: 1.2701 - val_loss: 2.1347 - val_mae: 2.1347\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.2532 - mae: 1.2532 - val_loss: 2.1377 - val_mae: 2.1377\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2790 - mae: 1.2790 - val_loss: 2.1002 - val_mae: 2.1002\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.2696 - mae: 1.2696 - val_loss: 2.1211 - val_mae: 2.1211\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.3056 - mae: 1.3056 - val_loss: 2.1012 - val_mae: 2.1012\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.2264 - mae: 1.2264 - val_loss: 2.1081 - val_mae: 2.1081\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.2636 - mae: 1.2636 - val_loss: 2.1044 - val_mae: 2.1044\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2145 - mae: 1.2145 - val_loss: 2.1505 - val_mae: 2.1505\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2078 - mae: 1.2078 - val_loss: 2.1243 - val_mae: 2.1243\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2272 - mae: 1.2272 - val_loss: 2.1596 - val_mae: 2.1596\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.2163 - mae: 1.2163 - val_loss: 2.1908 - val_mae: 2.1908\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2147 - mae: 1.2147 - val_loss: 2.0975 - val_mae: 2.0975\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.1961 - mae: 1.1961 - val_loss: 2.0788 - val_mae: 2.0788\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.2084 - mae: 1.2084 - val_loss: 2.1152 - val_mae: 2.1152\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2282 - mae: 1.2282 - val_loss: 2.0623 - val_mae: 2.0623\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2120 - mae: 1.2120 - val_loss: 2.1172 - val_mae: 2.1172\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2443 - mae: 1.2443 - val_loss: 2.1263 - val_mae: 2.1263\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2160 - mae: 1.2160 - val_loss: 2.1192 - val_mae: 2.1192\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2140 - mae: 1.2140 - val_loss: 2.1210 - val_mae: 2.1210\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3000 - mae: 1.3000 - val_loss: 2.0835 - val_mae: 2.0835\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2143 - mae: 1.2143 - val_loss: 2.0830 - val_mae: 2.0830\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2901 - mae: 1.2901 - val_loss: 2.1246 - val_mae: 2.1246\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2015 - mae: 1.2015 - val_loss: 2.0831 - val_mae: 2.0831\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1532 - mae: 1.1532 - val_loss: 2.0945 - val_mae: 2.0945\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1949 - mae: 1.1949 - val_loss: 2.1491 - val_mae: 2.1491\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1992 - mae: 1.1992 - val_loss: 2.1346 - val_mae: 2.1346\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2002 - mae: 1.2002 - val_loss: 2.1070 - val_mae: 2.1070\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1849 - mae: 1.1849 - val_loss: 2.1701 - val_mae: 2.1701\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2343 - mae: 1.2343 - val_loss: 2.1886 - val_mae: 2.1886\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2442 - mae: 1.2442 - val_loss: 2.1829 - val_mae: 2.1829\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1628 - mae: 1.1628 - val_loss: 2.0501 - val_mae: 2.0501\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.2013 - mae: 1.2013 - val_loss: 2.0977 - val_mae: 2.0977\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1641 - mae: 1.1641 - val_loss: 2.1184 - val_mae: 2.1184\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1341 - mae: 1.1341 - val_loss: 2.1121 - val_mae: 2.1121\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1840 - mae: 1.1840 - val_loss: 2.1269 - val_mae: 2.1269\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2137 - mae: 1.2137 - val_loss: 2.1581 - val_mae: 2.1581\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1922 - mae: 1.1922 - val_loss: 2.1120 - val_mae: 2.1120\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1264 - mae: 1.1264 - val_loss: 2.0757 - val_mae: 2.0757\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1490 - mae: 1.1490 - val_loss: 2.0909 - val_mae: 2.0909\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1301 - mae: 1.1301 - val_loss: 2.0784 - val_mae: 2.0784\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1697 - mae: 1.1697 - val_loss: 2.0649 - val_mae: 2.0649\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1629 - mae: 1.1629 - val_loss: 2.0644 - val_mae: 2.0644\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1269 - mae: 1.1269 - val_loss: 2.0794 - val_mae: 2.0794\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1136 - mae: 1.1136 - val_loss: 2.0913 - val_mae: 2.0913\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1242 - mae: 1.1242 - val_loss: 2.0762 - val_mae: 2.0762\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1217 - mae: 1.1217 - val_loss: 2.1212 - val_mae: 2.1212\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1077 - mae: 1.1077 - val_loss: 2.0561 - val_mae: 2.0561\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0913 - mae: 1.0913 - val_loss: 2.0798 - val_mae: 2.0798\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1064 - mae: 1.1064 - val_loss: 2.0619 - val_mae: 2.0619\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0935 - mae: 1.0935 - val_loss: 2.0465 - val_mae: 2.0465\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1020 - mae: 1.1020 - val_loss: 2.0773 - val_mae: 2.0773\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1232 - mae: 1.1232 - val_loss: 2.0232 - val_mae: 2.0232\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1455 - mae: 1.1455 - val_loss: 2.0420 - val_mae: 2.0420\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1229 - mae: 1.1229 - val_loss: 2.0787 - val_mae: 2.0787\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0802 - mae: 1.0802 - val_loss: 2.0627 - val_mae: 2.0627\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1145 - mae: 1.1145 - val_loss: 2.1024 - val_mae: 2.1024\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1976 - mae: 1.1976 - val_loss: 2.0725 - val_mae: 2.0725\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1908 - mae: 1.1908 - val_loss: 2.0208 - val_mae: 2.0208\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1324 - mae: 1.1324 - val_loss: 2.0438 - val_mae: 2.0438\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1437 - mae: 1.1437 - val_loss: 2.0847 - val_mae: 2.0847\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1467 - mae: 1.1467 - val_loss: 2.0366 - val_mae: 2.0366\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0846 - mae: 1.0846 - val_loss: 1.9958 - val_mae: 1.9958\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1560 - mae: 1.1560 - val_loss: 2.0232 - val_mae: 2.0232\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1757 - mae: 1.1757 - val_loss: 2.0662 - val_mae: 2.0662\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1180 - mae: 1.1180 - val_loss: 2.0554 - val_mae: 2.0554\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1358 - mae: 1.1358 - val_loss: 2.0625 - val_mae: 2.0625\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1254 - mae: 1.1254 - val_loss: 2.0990 - val_mae: 2.0990\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0486 - mae: 1.0486 - val_loss: 1.9835 - val_mae: 1.9835\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1876 - mae: 1.1876 - val_loss: 2.0124 - val_mae: 2.0124\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1110 - mae: 1.1110 - val_loss: 2.0548 - val_mae: 2.0548\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0810 - mae: 1.0810 - val_loss: 2.0092 - val_mae: 2.0092\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0608 - mae: 1.0608 - val_loss: 1.9748 - val_mae: 1.9748\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0495 - mae: 1.0495 - val_loss: 2.0148 - val_mae: 2.0148\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0381 - mae: 1.0381 - val_loss: 2.0159 - val_mae: 2.0159\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0233 - mae: 1.0233 - val_loss: 2.0539 - val_mae: 2.0539\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0505 - mae: 1.0505 - val_loss: 2.0628 - val_mae: 2.0628\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0797 - mae: 1.0797 - val_loss: 1.9861 - val_mae: 1.9861\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0518 - mae: 1.0518 - val_loss: 2.0530 - val_mae: 2.0530\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1326 - mae: 1.1326 - val_loss: 2.0145 - val_mae: 2.0145\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0616 - mae: 1.0616 - val_loss: 2.0366 - val_mae: 2.0366\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0815 - mae: 1.0815 - val_loss: 2.0244 - val_mae: 2.0244\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0181 - mae: 1.0181 - val_loss: 2.0121 - val_mae: 2.0121\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0732 - mae: 1.0732 - val_loss: 1.9429 - val_mae: 1.9429\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0903 - mae: 1.0903 - val_loss: 2.0059 - val_mae: 2.0059\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0464 - mae: 1.0464 - val_loss: 1.9722 - val_mae: 1.9722\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0179 - mae: 1.0179 - val_loss: 1.9940 - val_mae: 1.9940\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0154 - mae: 1.0154 - val_loss: 1.9503 - val_mae: 1.9503\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0735 - mae: 1.0735 - val_loss: 2.0086 - val_mae: 2.0086\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1511 - mae: 1.1511 - val_loss: 2.0527 - val_mae: 2.0527\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1052 - mae: 1.1052 - val_loss: 1.9707 - val_mae: 1.9707\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0481 - mae: 1.0481 - val_loss: 2.0541 - val_mae: 2.0541\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0359 - mae: 1.0359 - val_loss: 1.9552 - val_mae: 1.9552\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0582 - mae: 1.0582 - val_loss: 1.9727 - val_mae: 1.9727\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0504 - mae: 1.0504 - val_loss: 1.9678 - val_mae: 1.9678\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0711 - mae: 1.0711 - val_loss: 1.9315 - val_mae: 1.9315\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0777 - mae: 1.0777 - val_loss: 1.9474 - val_mae: 1.9474\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 1.0377 - mae: 1.0377 - val_loss: 2.0483 - val_mae: 2.0483\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.0448 - mae: 1.0448 - val_loss: 2.0108 - val_mae: 2.0108\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 1.0123 - mae: 1.0123 - val_loss: 1.9543 - val_mae: 1.9543\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.9980 - mae: 0.9980 - val_loss: 1.9525 - val_mae: 1.9525\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0027 - mae: 1.0027 - val_loss: 2.0035 - val_mae: 2.0035\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9830 - mae: 0.9830 - val_loss: 1.9680 - val_mae: 1.9680\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0086 - mae: 1.0086 - val_loss: 1.9930 - val_mae: 1.9930\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0003 - mae: 1.0003 - val_loss: 2.0038 - val_mae: 2.0038\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0159 - mae: 1.0159 - val_loss: 2.0001 - val_mae: 2.0001\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0014 - mae: 1.0014 - val_loss: 1.9773 - val_mae: 1.9773\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0015 - mae: 1.0015 - val_loss: 1.9927 - val_mae: 1.9927\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9819 - mae: 0.9819 - val_loss: 1.9876 - val_mae: 1.9876\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9798 - mae: 0.9798 - val_loss: 1.9288 - val_mae: 1.9288\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1qdGtBbn7N",
        "outputId": "232550bc-b253-41bb-fdbe-35b95ce3fdea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "source": [
        "errors = np.arange(1,251)\n",
        "\n",
        "mae = history.history['val_mae']\n",
        "\n",
        "plt.plot(errors, mae, c='#0095B6', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.show()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB",
        "outputId": "9d25b94f-ad8e-4416-dfe0-5ef5df4f442f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "history = model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 1s 17ms/step - loss: 8.0673 - mean_squared_logarithmic_error: 8.0673 - val_loss: 6.0317 - val_mean_squared_logarithmic_error: 6.0317\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 4.8822 - mean_squared_logarithmic_error: 4.8822 - val_loss: 3.5219 - val_mean_squared_logarithmic_error: 3.5219\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.8395 - mean_squared_logarithmic_error: 2.8395 - val_loss: 2.0043 - val_mean_squared_logarithmic_error: 2.0043\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6277 - mean_squared_logarithmic_error: 1.6277 - val_loss: 1.1209 - val_mean_squared_logarithmic_error: 1.1209\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9269 - mean_squared_logarithmic_error: 0.9269 - val_loss: 0.6412 - val_mean_squared_logarithmic_error: 0.6412\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5594 - mean_squared_logarithmic_error: 0.5594 - val_loss: 0.3819 - val_mean_squared_logarithmic_error: 0.3819\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.3540 - mean_squared_logarithmic_error: 0.3540 - val_loss: 0.2457 - val_mean_squared_logarithmic_error: 0.2457\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2448 - mean_squared_logarithmic_error: 0.2448 - val_loss: 0.1733 - val_mean_squared_logarithmic_error: 0.1733\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1841 - mean_squared_logarithmic_error: 0.1841 - val_loss: 0.1330 - val_mean_squared_logarithmic_error: 0.1330\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1484 - mean_squared_logarithmic_error: 0.1484 - val_loss: 0.1086 - val_mean_squared_logarithmic_error: 0.1086\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1261 - mean_squared_logarithmic_error: 0.1261 - val_loss: 0.0926 - val_mean_squared_logarithmic_error: 0.0926\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.1109 - mean_squared_logarithmic_error: 0.1109 - val_loss: 0.0822 - val_mean_squared_logarithmic_error: 0.0822\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.1006 - mean_squared_logarithmic_error: 0.1006 - val_loss: 0.0738 - val_mean_squared_logarithmic_error: 0.0738\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0918 - mean_squared_logarithmic_error: 0.0918 - val_loss: 0.0674 - val_mean_squared_logarithmic_error: 0.0674\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0854 - mean_squared_logarithmic_error: 0.0854 - val_loss: 0.0623 - val_mean_squared_logarithmic_error: 0.0623\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0795 - mean_squared_logarithmic_error: 0.0795 - val_loss: 0.0579 - val_mean_squared_logarithmic_error: 0.0579\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0746 - mean_squared_logarithmic_error: 0.0746 - val_loss: 0.0540 - val_mean_squared_logarithmic_error: 0.0540\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0702 - mean_squared_logarithmic_error: 0.0702 - val_loss: 0.0504 - val_mean_squared_logarithmic_error: 0.0504\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0661 - mean_squared_logarithmic_error: 0.0661 - val_loss: 0.0474 - val_mean_squared_logarithmic_error: 0.0474\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0628 - mean_squared_logarithmic_error: 0.0628 - val_loss: 0.0446 - val_mean_squared_logarithmic_error: 0.0446\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0595 - mean_squared_logarithmic_error: 0.0595 - val_loss: 0.0423 - val_mean_squared_logarithmic_error: 0.0423\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0566 - mean_squared_logarithmic_error: 0.0566 - val_loss: 0.0401 - val_mean_squared_logarithmic_error: 0.0401\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0539 - mean_squared_logarithmic_error: 0.0539 - val_loss: 0.0384 - val_mean_squared_logarithmic_error: 0.0384\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0516 - mean_squared_logarithmic_error: 0.0516 - val_loss: 0.0367 - val_mean_squared_logarithmic_error: 0.0367\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0494 - mean_squared_logarithmic_error: 0.0494 - val_loss: 0.0352 - val_mean_squared_logarithmic_error: 0.0352\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0475 - mean_squared_logarithmic_error: 0.0475 - val_loss: 0.0338 - val_mean_squared_logarithmic_error: 0.0338\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0456 - mean_squared_logarithmic_error: 0.0456 - val_loss: 0.0326 - val_mean_squared_logarithmic_error: 0.0326\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0439 - mean_squared_logarithmic_error: 0.0439 - val_loss: 0.0313 - val_mean_squared_logarithmic_error: 0.0313\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0425 - mean_squared_logarithmic_error: 0.0425 - val_loss: 0.0301 - val_mean_squared_logarithmic_error: 0.0301\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0411 - mean_squared_logarithmic_error: 0.0411 - val_loss: 0.0294 - val_mean_squared_logarithmic_error: 0.0294\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.0399 - mean_squared_logarithmic_error: 0.0399 - val_loss: 0.0287 - val_mean_squared_logarithmic_error: 0.0287\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0388 - mean_squared_logarithmic_error: 0.0388 - val_loss: 0.0283 - val_mean_squared_logarithmic_error: 0.0283\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0379 - mean_squared_logarithmic_error: 0.0379 - val_loss: 0.0275 - val_mean_squared_logarithmic_error: 0.0275\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0369 - mean_squared_logarithmic_error: 0.0369 - val_loss: 0.0272 - val_mean_squared_logarithmic_error: 0.0272\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0360 - mean_squared_logarithmic_error: 0.0360 - val_loss: 0.0264 - val_mean_squared_logarithmic_error: 0.0264\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0352 - mean_squared_logarithmic_error: 0.0352 - val_loss: 0.0262 - val_mean_squared_logarithmic_error: 0.0262\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0346 - mean_squared_logarithmic_error: 0.0346 - val_loss: 0.0259 - val_mean_squared_logarithmic_error: 0.0259\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0338 - mean_squared_logarithmic_error: 0.0338 - val_loss: 0.0257 - val_mean_squared_logarithmic_error: 0.0257\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0333 - mean_squared_logarithmic_error: 0.0333 - val_loss: 0.0256 - val_mean_squared_logarithmic_error: 0.0256\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0326 - mean_squared_logarithmic_error: 0.0326 - val_loss: 0.0250 - val_mean_squared_logarithmic_error: 0.0250\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0323 - mean_squared_logarithmic_error: 0.0323 - val_loss: 0.0245 - val_mean_squared_logarithmic_error: 0.0245\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0316 - mean_squared_logarithmic_error: 0.0316 - val_loss: 0.0246 - val_mean_squared_logarithmic_error: 0.0246\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0311 - mean_squared_logarithmic_error: 0.0311 - val_loss: 0.0242 - val_mean_squared_logarithmic_error: 0.0242\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0306 - mean_squared_logarithmic_error: 0.0306 - val_loss: 0.0242 - val_mean_squared_logarithmic_error: 0.0242\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0302 - mean_squared_logarithmic_error: 0.0302 - val_loss: 0.0237 - val_mean_squared_logarithmic_error: 0.0237\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0298 - mean_squared_logarithmic_error: 0.0298 - val_loss: 0.0237 - val_mean_squared_logarithmic_error: 0.0237\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0294 - mean_squared_logarithmic_error: 0.0294 - val_loss: 0.0236 - val_mean_squared_logarithmic_error: 0.0236\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0291 - mean_squared_logarithmic_error: 0.0291 - val_loss: 0.0237 - val_mean_squared_logarithmic_error: 0.0237\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0287 - mean_squared_logarithmic_error: 0.0287 - val_loss: 0.0232 - val_mean_squared_logarithmic_error: 0.0232\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0283 - mean_squared_logarithmic_error: 0.0283 - val_loss: 0.0230 - val_mean_squared_logarithmic_error: 0.0230\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0280 - mean_squared_logarithmic_error: 0.0280 - val_loss: 0.0227 - val_mean_squared_logarithmic_error: 0.0227\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0277 - mean_squared_logarithmic_error: 0.0277 - val_loss: 0.0226 - val_mean_squared_logarithmic_error: 0.0226\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0274 - mean_squared_logarithmic_error: 0.0274 - val_loss: 0.0226 - val_mean_squared_logarithmic_error: 0.0226\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0271 - mean_squared_logarithmic_error: 0.0271 - val_loss: 0.0222 - val_mean_squared_logarithmic_error: 0.0222\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0269 - mean_squared_logarithmic_error: 0.0269 - val_loss: 0.0222 - val_mean_squared_logarithmic_error: 0.0222\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0265 - mean_squared_logarithmic_error: 0.0265 - val_loss: 0.0218 - val_mean_squared_logarithmic_error: 0.0218\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0263 - mean_squared_logarithmic_error: 0.0263 - val_loss: 0.0220 - val_mean_squared_logarithmic_error: 0.0220\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0261 - mean_squared_logarithmic_error: 0.0261 - val_loss: 0.0223 - val_mean_squared_logarithmic_error: 0.0223\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0256 - mean_squared_logarithmic_error: 0.0256 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0254 - mean_squared_logarithmic_error: 0.0254 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0252 - mean_squared_logarithmic_error: 0.0252 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0251 - mean_squared_logarithmic_error: 0.0251 - val_loss: 0.0218 - val_mean_squared_logarithmic_error: 0.0218\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0248 - mean_squared_logarithmic_error: 0.0248 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0246 - mean_squared_logarithmic_error: 0.0246 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0243 - mean_squared_logarithmic_error: 0.0243 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0242 - mean_squared_logarithmic_error: 0.0242 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0240 - mean_squared_logarithmic_error: 0.0240 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0238 - mean_squared_logarithmic_error: 0.0238 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0237 - mean_squared_logarithmic_error: 0.0237 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0234 - mean_squared_logarithmic_error: 0.0234 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0232 - mean_squared_logarithmic_error: 0.0232 - val_loss: 0.0205 - val_mean_squared_logarithmic_error: 0.0205\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0231 - mean_squared_logarithmic_error: 0.0231 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0229 - mean_squared_logarithmic_error: 0.0229 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0228 - mean_squared_logarithmic_error: 0.0228 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0226 - mean_squared_logarithmic_error: 0.0226 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0224 - mean_squared_logarithmic_error: 0.0224 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.0223 - mean_squared_logarithmic_error: 0.0223 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0222 - mean_squared_logarithmic_error: 0.0222 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0220 - mean_squared_logarithmic_error: 0.0220 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0219 - mean_squared_logarithmic_error: 0.0219 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0218 - mean_squared_logarithmic_error: 0.0218 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0216 - mean_squared_logarithmic_error: 0.0216 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0215 - mean_squared_logarithmic_error: 0.0215 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0214 - mean_squared_logarithmic_error: 0.0214 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0212 - mean_squared_logarithmic_error: 0.0212 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0211 - mean_squared_logarithmic_error: 0.0211 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0209 - mean_squared_logarithmic_error: 0.0209 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0207 - mean_squared_logarithmic_error: 0.0207 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0207 - mean_squared_logarithmic_error: 0.0207 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0206 - mean_squared_logarithmic_error: 0.0206 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0204 - mean_squared_logarithmic_error: 0.0204 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0204 - mean_squared_logarithmic_error: 0.0204 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0202 - mean_squared_logarithmic_error: 0.0202 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0201 - mean_squared_logarithmic_error: 0.0201 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0200 - mean_squared_logarithmic_error: 0.0200 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0198 - mean_squared_logarithmic_error: 0.0198 - val_loss: 0.0193 - val_mean_squared_logarithmic_error: 0.0193\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0197 - mean_squared_logarithmic_error: 0.0197 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0195 - mean_squared_logarithmic_error: 0.0195 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0194 - mean_squared_logarithmic_error: 0.0194 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0194 - mean_squared_logarithmic_error: 0.0194 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0193 - mean_squared_logarithmic_error: 0.0193 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0193 - mean_squared_logarithmic_error: 0.0193 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0192 - mean_squared_logarithmic_error: 0.0192 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0190 - mean_squared_logarithmic_error: 0.0190 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0187 - mean_squared_logarithmic_error: 0.0187 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0187 - mean_squared_logarithmic_error: 0.0187 - val_loss: 0.0179 - val_mean_squared_logarithmic_error: 0.0179\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0185 - mean_squared_logarithmic_error: 0.0185 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0185 - mean_squared_logarithmic_error: 0.0185 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0184 - mean_squared_logarithmic_error: 0.0184 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0181 - mean_squared_logarithmic_error: 0.0181 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0176 - mean_squared_logarithmic_error: 0.0176 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0175 - mean_squared_logarithmic_error: 0.0175 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0176 - mean_squared_logarithmic_error: 0.0176 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0173 - mean_squared_logarithmic_error: 0.0173 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0173 - mean_squared_logarithmic_error: 0.0173 - val_loss: 0.0179 - val_mean_squared_logarithmic_error: 0.0179\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0173 - mean_squared_logarithmic_error: 0.0173 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0169 - mean_squared_logarithmic_error: 0.0169 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0169 - mean_squared_logarithmic_error: 0.0169 - val_loss: 0.0169 - val_mean_squared_logarithmic_error: 0.0169\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0170 - val_mean_squared_logarithmic_error: 0.0170\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0175 - val_mean_squared_logarithmic_error: 0.0175\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0163 - mean_squared_logarithmic_error: 0.0163 - val_loss: 0.0177 - val_mean_squared_logarithmic_error: 0.0177\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0172 - val_mean_squared_logarithmic_error: 0.0172\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0178 - val_mean_squared_logarithmic_error: 0.0178\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0176 - val_mean_squared_logarithmic_error: 0.0176\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0161 - mean_squared_logarithmic_error: 0.0161 - val_loss: 0.0168 - val_mean_squared_logarithmic_error: 0.0168\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0169 - val_mean_squared_logarithmic_error: 0.0169\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0170 - val_mean_squared_logarithmic_error: 0.0170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = np.arange(1,151)\n",
        "\n",
        "msle = history.history['val_mean_squared_logarithmic_error']\n",
        "\n",
        "plt.plot(epochs, msle, c='#F47789', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "g-77hz_V9VsQ",
        "outputId": "eeb57f60-0650-4112-c47c-8f002a69a30f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aX5w42vRXMWj"
      },
      "source": [
        "### Answer 4\n",
        "\n",
        "because the logarithmic of 0 in unkown which will output -inf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IOUf7iOkYxcM"
      },
      "source": [
        "### Answer 5\n",
        "\n",
        "**MSE** :  also known as L2 loss. we can use it when  our project requires avoiding large errors as much as possible\n",
        "\n",
        "**MAE** :  also known as L1 loss. It's more robust to outliers since it does not make use of square.\n",
        "\n",
        "**MSLE** :  It only cares about the percentual difference. That means It will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_9nfLOffJ_P"
      },
      "source": [
        "### Answer 6"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWlkms1flkZ",
        "outputId": "accf4fac-abf2-43f9-e52b-ecc9fd41757f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss= tf.keras.losses.Huber(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 5s 21ms/step - loss: 21.4710 - accuracy: 0.0000e+00 - val_loss: 19.9464 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 20.0772 - accuracy: 0.0000e+00 - val_loss: 18.1061 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 17.5244 - accuracy: 0.0000e+00 - val_loss: 14.8605 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.5495 - accuracy: 0.0000e+00 - val_loss: 10.4458 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.7436 - accuracy: 0.0000e+00 - val_loss: 5.7936 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.4721 - accuracy: 0.0000e+00 - val_loss: 5.0408 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 4.1973 - accuracy: 0.0000e+00 - val_loss: 3.5188 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 3.1372 - accuracy: 0.0000e+00 - val_loss: 3.1168 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.8816 - accuracy: 0.0000e+00 - val_loss: 2.7519 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.7496 - accuracy: 0.0000e+00 - val_loss: 2.8536 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.5908 - accuracy: 0.0000e+00 - val_loss: 2.5454 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 2.4571 - accuracy: 0.0000e+00 - val_loss: 2.4550 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.3195 - accuracy: 0.0000e+00 - val_loss: 2.3590 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.2418 - accuracy: 0.0000e+00 - val_loss: 2.2224 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.1688 - accuracy: 0.0000e+00 - val_loss: 2.1902 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0972 - accuracy: 0.0000e+00 - val_loss: 2.1525 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 2.0400 - accuracy: 0.0000e+00 - val_loss: 2.1020 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9965 - accuracy: 0.0000e+00 - val_loss: 2.0643 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.9325 - accuracy: 0.0000e+00 - val_loss: 2.0663 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8980 - accuracy: 0.0000e+00 - val_loss: 1.8996 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8542 - accuracy: 0.0000e+00 - val_loss: 2.0258 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8415 - accuracy: 0.0000e+00 - val_loss: 1.8865 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.8050 - accuracy: 0.0000e+00 - val_loss: 1.8689 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8802 - accuracy: 0.0000e+00 - val_loss: 1.9176 - val_accuracy: 0.0000e+00\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7597 - accuracy: 0.0000e+00 - val_loss: 1.8885 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7210 - accuracy: 0.0000e+00 - val_loss: 1.7207 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.7162 - accuracy: 0.0000e+00 - val_loss: 1.8506 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6955 - accuracy: 0.0000e+00 - val_loss: 1.7958 - val_accuracy: 0.0000e+00\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.6874 - accuracy: 0.0000e+00 - val_loss: 1.7395 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6614 - accuracy: 0.0000e+00 - val_loss: 1.8361 - val_accuracy: 0.0000e+00\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6373 - accuracy: 0.0000e+00 - val_loss: 1.7896 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.6206 - accuracy: 0.0000e+00 - val_loss: 1.6837 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5896 - accuracy: 0.0000e+00 - val_loss: 1.8186 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5749 - accuracy: 0.0000e+00 - val_loss: 1.6559 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5860 - accuracy: 0.0000e+00 - val_loss: 1.7682 - val_accuracy: 0.0000e+00\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5573 - accuracy: 0.0000e+00 - val_loss: 1.6842 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5020 - accuracy: 0.0000e+00 - val_loss: 1.6613 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.5360 - accuracy: 0.0000e+00 - val_loss: 1.7437 - val_accuracy: 0.0000e+00\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5747 - accuracy: 0.0000e+00 - val_loss: 1.6817 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5135 - accuracy: 0.0000e+00 - val_loss: 1.5655 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5060 - accuracy: 0.0000e+00 - val_loss: 1.7975 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4695 - accuracy: 0.0000e+00 - val_loss: 1.5246 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4914 - accuracy: 0.0000e+00 - val_loss: 1.7931 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.5408 - accuracy: 0.0000e+00 - val_loss: 1.5500 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4856 - accuracy: 0.0000e+00 - val_loss: 1.6953 - val_accuracy: 0.0000e+00\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4430 - accuracy: 0.0000e+00 - val_loss: 1.5930 - val_accuracy: 0.0000e+00\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3996 - accuracy: 0.0000e+00 - val_loss: 1.5722 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3600 - accuracy: 0.0000e+00 - val_loss: 1.5751 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.3698 - accuracy: 0.0000e+00 - val_loss: 1.5645 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3745 - accuracy: 0.0000e+00 - val_loss: 1.7323 - val_accuracy: 0.0000e+00\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3679 - accuracy: 0.0000e+00 - val_loss: 1.5650 - val_accuracy: 0.0000e+00\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3461 - accuracy: 0.0000e+00 - val_loss: 1.5720 - val_accuracy: 0.0000e+00\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2944 - accuracy: 0.0000e+00 - val_loss: 1.5976 - val_accuracy: 0.0000e+00\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3365 - accuracy: 0.0000e+00 - val_loss: 1.5993 - val_accuracy: 0.0000e+00\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2775 - accuracy: 0.0000e+00 - val_loss: 1.6593 - val_accuracy: 0.0000e+00\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2696 - accuracy: 0.0000e+00 - val_loss: 1.6153 - val_accuracy: 0.0000e+00\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2364 - accuracy: 0.0000e+00 - val_loss: 1.5269 - val_accuracy: 0.0000e+00\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.2441 - accuracy: 0.0000e+00 - val_loss: 1.6127 - val_accuracy: 0.0000e+00\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2431 - accuracy: 0.0000e+00 - val_loss: 1.6144 - val_accuracy: 0.0000e+00\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.2006 - accuracy: 0.0000e+00 - val_loss: 1.5739 - val_accuracy: 0.0000e+00\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1741 - accuracy: 0.0000e+00 - val_loss: 1.5938 - val_accuracy: 0.0000e+00\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.1618 - accuracy: 0.0000e+00 - val_loss: 1.5193 - val_accuracy: 0.0000e+00\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1561 - accuracy: 0.0000e+00 - val_loss: 1.6205 - val_accuracy: 0.0000e+00\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1730 - accuracy: 0.0000e+00 - val_loss: 1.5147 - val_accuracy: 0.0000e+00\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1195 - accuracy: 0.0000e+00 - val_loss: 1.5463 - val_accuracy: 0.0000e+00\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.1119 - accuracy: 0.0000e+00 - val_loss: 1.5464 - val_accuracy: 0.0000e+00\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0914 - accuracy: 0.0000e+00 - val_loss: 1.5087 - val_accuracy: 0.0000e+00\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.1133 - accuracy: 0.0000e+00 - val_loss: 1.5111 - val_accuracy: 0.0000e+00\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0899 - accuracy: 0.0000e+00 - val_loss: 1.5062 - val_accuracy: 0.0000e+00\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0633 - accuracy: 0.0000e+00 - val_loss: 1.4752 - val_accuracy: 0.0000e+00\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0677 - accuracy: 0.0000e+00 - val_loss: 1.5323 - val_accuracy: 0.0000e+00\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0946 - accuracy: 0.0000e+00 - val_loss: 1.5336 - val_accuracy: 0.0000e+00\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0671 - accuracy: 0.0000e+00 - val_loss: 1.5337 - val_accuracy: 0.0000e+00\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0606 - accuracy: 0.0000e+00 - val_loss: 1.4587 - val_accuracy: 0.0000e+00\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 1.0493 - accuracy: 0.0000e+00 - val_loss: 1.5012 - val_accuracy: 0.0000e+00\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 1.0591 - accuracy: 0.0000e+00 - val_loss: 1.4583 - val_accuracy: 0.0000e+00\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 1.0446 - accuracy: 0.0000e+00 - val_loss: 1.5520 - val_accuracy: 0.0000e+00\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0437 - accuracy: 0.0000e+00 - val_loss: 1.4605 - val_accuracy: 0.0000e+00\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0257 - accuracy: 0.0000e+00 - val_loss: 1.4775 - val_accuracy: 0.0000e+00\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0325 - accuracy: 0.0000e+00 - val_loss: 1.4788 - val_accuracy: 0.0000e+00\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0557 - accuracy: 0.0000e+00 - val_loss: 1.4197 - val_accuracy: 0.0000e+00\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0008 - accuracy: 0.0000e+00 - val_loss: 1.4604 - val_accuracy: 0.0000e+00\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9926 - accuracy: 0.0000e+00 - val_loss: 1.4294 - val_accuracy: 0.0000e+00\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9854 - accuracy: 0.0000e+00 - val_loss: 1.3576 - val_accuracy: 0.0000e+00\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.0141 - accuracy: 0.0000e+00 - val_loss: 1.5287 - val_accuracy: 0.0000e+00\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9819 - accuracy: 0.0000e+00 - val_loss: 1.4415 - val_accuracy: 0.0000e+00\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9713 - accuracy: 0.0000e+00 - val_loss: 1.4395 - val_accuracy: 0.0000e+00\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9606 - accuracy: 0.0000e+00 - val_loss: 1.4387 - val_accuracy: 0.0000e+00\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9535 - accuracy: 0.0000e+00 - val_loss: 1.4245 - val_accuracy: 0.0000e+00\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9679 - accuracy: 0.0000e+00 - val_loss: 1.4034 - val_accuracy: 0.0000e+00\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9653 - accuracy: 0.0000e+00 - val_loss: 1.4504 - val_accuracy: 0.0000e+00\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9493 - accuracy: 0.0000e+00 - val_loss: 1.3706 - val_accuracy: 0.0000e+00\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9041 - accuracy: 0.0000e+00 - val_loss: 1.4092 - val_accuracy: 0.0000e+00\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9135 - accuracy: 0.0000e+00 - val_loss: 1.3698 - val_accuracy: 0.0000e+00\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9429 - accuracy: 0.0000e+00 - val_loss: 1.3816 - val_accuracy: 0.0000e+00\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9229 - accuracy: 0.0000e+00 - val_loss: 1.3404 - val_accuracy: 0.0000e+00\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9179 - accuracy: 0.0000e+00 - val_loss: 1.3354 - val_accuracy: 0.0000e+00\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8867 - accuracy: 0.0000e+00 - val_loss: 1.3663 - val_accuracy: 0.0000e+00\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8956 - accuracy: 0.0000e+00 - val_loss: 1.3201 - val_accuracy: 0.0000e+00\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8980 - accuracy: 0.0000e+00 - val_loss: 1.3326 - val_accuracy: 0.0000e+00\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8948 - accuracy: 0.0000e+00 - val_loss: 1.3137 - val_accuracy: 0.0000e+00\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8622 - accuracy: 0.0000e+00 - val_loss: 1.3384 - val_accuracy: 0.0000e+00\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9001 - accuracy: 0.0000e+00 - val_loss: 1.3230 - val_accuracy: 0.0000e+00\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9406 - accuracy: 0.0000e+00 - val_loss: 1.3111 - val_accuracy: 0.0000e+00\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9016 - accuracy: 0.0000e+00 - val_loss: 1.2688 - val_accuracy: 0.0000e+00\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8445 - accuracy: 0.0000e+00 - val_loss: 1.2879 - val_accuracy: 0.0000e+00\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8666 - accuracy: 0.0000e+00 - val_loss: 1.2845 - val_accuracy: 0.0000e+00\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8855 - accuracy: 0.0000e+00 - val_loss: 1.3103 - val_accuracy: 0.0000e+00\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8767 - accuracy: 0.0000e+00 - val_loss: 1.2355 - val_accuracy: 0.0000e+00\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8589 - accuracy: 0.0000e+00 - val_loss: 1.2464 - val_accuracy: 0.0000e+00\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8605 - accuracy: 0.0000e+00 - val_loss: 1.1999 - val_accuracy: 0.0000e+00\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9052 - accuracy: 0.0000e+00 - val_loss: 1.3057 - val_accuracy: 0.0000e+00\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8688 - accuracy: 0.0000e+00 - val_loss: 1.3182 - val_accuracy: 0.0000e+00\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.9052 - accuracy: 0.0000e+00 - val_loss: 1.2473 - val_accuracy: 0.0000e+00\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8353 - accuracy: 0.0000e+00 - val_loss: 1.2520 - val_accuracy: 0.0000e+00\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8274 - accuracy: 0.0000e+00 - val_loss: 1.2875 - val_accuracy: 0.0000e+00\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8384 - accuracy: 0.0000e+00 - val_loss: 1.2129 - val_accuracy: 0.0000e+00\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7956 - accuracy: 0.0000e+00 - val_loss: 1.2408 - val_accuracy: 0.0000e+00\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8004 - accuracy: 0.0000e+00 - val_loss: 1.2229 - val_accuracy: 0.0000e+00\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8258 - accuracy: 0.0000e+00 - val_loss: 1.2050 - val_accuracy: 0.0000e+00\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7822 - accuracy: 0.0000e+00 - val_loss: 1.2004 - val_accuracy: 0.0000e+00\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8128 - accuracy: 0.0000e+00 - val_loss: 1.2221 - val_accuracy: 0.0000e+00\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7830 - accuracy: 0.0000e+00 - val_loss: 1.2117 - val_accuracy: 0.0000e+00\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.8336 - accuracy: 0.0000e+00 - val_loss: 1.2456 - val_accuracy: 0.0000e+00\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8016 - accuracy: 0.0000e+00 - val_loss: 1.2073 - val_accuracy: 0.0000e+00\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7830 - accuracy: 0.0000e+00 - val_loss: 1.2139 - val_accuracy: 0.0000e+00\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7762 - accuracy: 0.0000e+00 - val_loss: 1.2325 - val_accuracy: 0.0000e+00\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7654 - accuracy: 0.0000e+00 - val_loss: 1.1743 - val_accuracy: 0.0000e+00\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7641 - accuracy: 0.0000e+00 - val_loss: 1.2002 - val_accuracy: 0.0000e+00\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7817 - accuracy: 0.0000e+00 - val_loss: 1.2426 - val_accuracy: 0.0000e+00\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.8120 - accuracy: 0.0000e+00 - val_loss: 1.1749 - val_accuracy: 0.0000e+00\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7406 - accuracy: 0.0000e+00 - val_loss: 1.2054 - val_accuracy: 0.0000e+00\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7599 - accuracy: 0.0000e+00 - val_loss: 1.2054 - val_accuracy: 0.0000e+00\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7899 - accuracy: 0.0000e+00 - val_loss: 1.1807 - val_accuracy: 0.0000e+00\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7890 - accuracy: 0.0000e+00 - val_loss: 1.2107 - val_accuracy: 0.0000e+00\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7232 - accuracy: 0.0000e+00 - val_loss: 1.1299 - val_accuracy: 0.0000e+00\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7251 - accuracy: 0.0000e+00 - val_loss: 1.1230 - val_accuracy: 0.0000e+00\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.7183 - accuracy: 0.0000e+00 - val_loss: 1.1887 - val_accuracy: 0.0000e+00\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7025 - accuracy: 0.0000e+00 - val_loss: 1.1752 - val_accuracy: 0.0000e+00\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7118 - accuracy: 0.0000e+00 - val_loss: 1.2251 - val_accuracy: 0.0000e+00\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6809 - accuracy: 0.0000e+00 - val_loss: 1.1660 - val_accuracy: 0.0000e+00\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7147 - accuracy: 0.0000e+00 - val_loss: 1.1904 - val_accuracy: 0.0000e+00\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6973 - accuracy: 0.0000e+00 - val_loss: 1.1146 - val_accuracy: 0.0000e+00\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6949 - accuracy: 0.0000e+00 - val_loss: 1.1194 - val_accuracy: 0.0000e+00\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.7102 - accuracy: 0.0000e+00 - val_loss: 1.2574 - val_accuracy: 0.0000e+00\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.7085 - accuracy: 0.0000e+00 - val_loss: 1.1964 - val_accuracy: 0.0000e+00\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6870 - accuracy: 0.0000e+00 - val_loss: 1.2033 - val_accuracy: 0.0000e+00\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6988 - accuracy: 0.0000e+00 - val_loss: 1.1435 - val_accuracy: 0.0000e+00\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6517 - accuracy: 0.0000e+00 - val_loss: 1.1448 - val_accuracy: 0.0000e+00\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6545 - accuracy: 0.0000e+00 - val_loss: 1.1410 - val_accuracy: 0.0000e+00\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6625 - accuracy: 0.0000e+00 - val_loss: 1.1892 - val_accuracy: 0.0000e+00\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6510 - accuracy: 0.0000e+00 - val_loss: 1.1591 - val_accuracy: 0.0000e+00\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6602 - accuracy: 0.0000e+00 - val_loss: 1.1757 - val_accuracy: 0.0000e+00\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6416 - accuracy: 0.0000e+00 - val_loss: 1.2085 - val_accuracy: 0.0000e+00\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6423 - accuracy: 0.0000e+00 - val_loss: 1.2262 - val_accuracy: 0.0000e+00\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6345 - accuracy: 0.0000e+00 - val_loss: 1.2140 - val_accuracy: 0.0000e+00\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6133 - accuracy: 0.0000e+00 - val_loss: 1.1745 - val_accuracy: 0.0000e+00\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.6385 - accuracy: 0.0000e+00 - val_loss: 1.1194 - val_accuracy: 0.0000e+00\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6330 - accuracy: 0.0000e+00 - val_loss: 1.1739 - val_accuracy: 0.0000e+00\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.6136 - accuracy: 0.0000e+00 - val_loss: 1.1469 - val_accuracy: 0.0000e+00\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6484 - accuracy: 0.0000e+00 - val_loss: 1.1642 - val_accuracy: 0.0000e+00\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6128 - accuracy: 0.0000e+00 - val_loss: 1.1987 - val_accuracy: 0.0000e+00\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6181 - accuracy: 0.0000e+00 - val_loss: 1.1584 - val_accuracy: 0.0000e+00\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6310 - accuracy: 0.0000e+00 - val_loss: 1.1360 - val_accuracy: 0.0000e+00\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6112 - accuracy: 0.0000e+00 - val_loss: 1.1570 - val_accuracy: 0.0000e+00\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6187 - accuracy: 0.0000e+00 - val_loss: 1.1489 - val_accuracy: 0.0000e+00\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6200 - accuracy: 0.0000e+00 - val_loss: 1.3076 - val_accuracy: 0.0000e+00\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6416 - accuracy: 0.0000e+00 - val_loss: 1.1523 - val_accuracy: 0.0000e+00\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5985 - accuracy: 0.0000e+00 - val_loss: 1.1593 - val_accuracy: 0.0000e+00\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5814 - accuracy: 0.0000e+00 - val_loss: 1.2577 - val_accuracy: 0.0000e+00\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6155 - accuracy: 0.0000e+00 - val_loss: 1.2455 - val_accuracy: 0.0000e+00\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6226 - accuracy: 0.0000e+00 - val_loss: 1.1985 - val_accuracy: 0.0000e+00\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5875 - accuracy: 0.0000e+00 - val_loss: 1.1643 - val_accuracy: 0.0000e+00\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5755 - accuracy: 0.0000e+00 - val_loss: 1.1372 - val_accuracy: 0.0000e+00\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.6008 - accuracy: 0.0000e+00 - val_loss: 1.2104 - val_accuracy: 0.0000e+00\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5632 - accuracy: 0.0000e+00 - val_loss: 1.2196 - val_accuracy: 0.0000e+00\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5973 - accuracy: 0.0000e+00 - val_loss: 1.2272 - val_accuracy: 0.0000e+00\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5920 - accuracy: 0.0000e+00 - val_loss: 1.1558 - val_accuracy: 0.0000e+00\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5329 - accuracy: 0.0000e+00 - val_loss: 1.1952 - val_accuracy: 0.0000e+00\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5495 - accuracy: 0.0000e+00 - val_loss: 1.1852 - val_accuracy: 0.0000e+00\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5301 - accuracy: 0.0000e+00 - val_loss: 1.1913 - val_accuracy: 0.0000e+00\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5373 - accuracy: 0.0000e+00 - val_loss: 1.1743 - val_accuracy: 0.0000e+00\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5489 - accuracy: 0.0000e+00 - val_loss: 1.2314 - val_accuracy: 0.0000e+00\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5531 - accuracy: 0.0000e+00 - val_loss: 1.1241 - val_accuracy: 0.0000e+00\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5447 - accuracy: 0.0000e+00 - val_loss: 1.1515 - val_accuracy: 0.0000e+00\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5344 - accuracy: 0.0000e+00 - val_loss: 1.2243 - val_accuracy: 0.0000e+00\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5490 - accuracy: 0.0000e+00 - val_loss: 1.2138 - val_accuracy: 0.0000e+00\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5180 - accuracy: 0.0000e+00 - val_loss: 1.2033 - val_accuracy: 0.0000e+00\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5235 - accuracy: 0.0000e+00 - val_loss: 1.1867 - val_accuracy: 0.0000e+00\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5024 - accuracy: 0.0000e+00 - val_loss: 1.1814 - val_accuracy: 0.0000e+00\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5101 - accuracy: 0.0000e+00 - val_loss: 1.1705 - val_accuracy: 0.0000e+00\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5288 - accuracy: 0.0000e+00 - val_loss: 1.1979 - val_accuracy: 0.0000e+00\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5465 - accuracy: 0.0000e+00 - val_loss: 1.1772 - val_accuracy: 0.0000e+00\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5003 - accuracy: 0.0000e+00 - val_loss: 1.1735 - val_accuracy: 0.0000e+00\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5040 - accuracy: 0.0000e+00 - val_loss: 1.2038 - val_accuracy: 0.0000e+00\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4845 - accuracy: 0.0000e+00 - val_loss: 1.2068 - val_accuracy: 0.0000e+00\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4850 - accuracy: 0.0000e+00 - val_loss: 1.2000 - val_accuracy: 0.0000e+00\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4955 - accuracy: 0.0000e+00 - val_loss: 1.2475 - val_accuracy: 0.0000e+00\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4809 - accuracy: 0.0000e+00 - val_loss: 1.2290 - val_accuracy: 0.0000e+00\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.5082 - accuracy: 0.0000e+00 - val_loss: 1.2570 - val_accuracy: 0.0000e+00\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.5183 - accuracy: 0.0000e+00 - val_loss: 1.2326 - val_accuracy: 0.0000e+00\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4850 - accuracy: 0.0000e+00 - val_loss: 1.1903 - val_accuracy: 0.0000e+00\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4811 - accuracy: 0.0000e+00 - val_loss: 1.2652 - val_accuracy: 0.0000e+00\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5353 - accuracy: 0.0000e+00 - val_loss: 1.2410 - val_accuracy: 0.0000e+00\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5537 - accuracy: 0.0000e+00 - val_loss: 1.1983 - val_accuracy: 0.0000e+00\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4844 - accuracy: 0.0000e+00 - val_loss: 1.1963 - val_accuracy: 0.0000e+00\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4787 - accuracy: 0.0000e+00 - val_loss: 1.2248 - val_accuracy: 0.0000e+00\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4736 - accuracy: 0.0000e+00 - val_loss: 1.2307 - val_accuracy: 0.0000e+00\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.5149 - accuracy: 0.0000e+00 - val_loss: 1.2008 - val_accuracy: 0.0000e+00\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.5035 - accuracy: 0.0000e+00 - val_loss: 1.1904 - val_accuracy: 0.0000e+00\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5136 - accuracy: 0.0000e+00 - val_loss: 1.1749 - val_accuracy: 0.0000e+00\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4917 - accuracy: 0.0000e+00 - val_loss: 1.2332 - val_accuracy: 0.0000e+00\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4818 - accuracy: 0.0000e+00 - val_loss: 1.1833 - val_accuracy: 0.0000e+00\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4867 - accuracy: 0.0000e+00 - val_loss: 1.3276 - val_accuracy: 0.0000e+00\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.5399 - accuracy: 0.0000e+00 - val_loss: 1.1947 - val_accuracy: 0.0000e+00\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.5542 - accuracy: 0.0000e+00 - val_loss: 1.1601 - val_accuracy: 0.0000e+00\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4957 - accuracy: 0.0000e+00 - val_loss: 1.2075 - val_accuracy: 0.0000e+00\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4943 - accuracy: 0.0000e+00 - val_loss: 1.2207 - val_accuracy: 0.0000e+00\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4690 - accuracy: 0.0000e+00 - val_loss: 1.2005 - val_accuracy: 0.0000e+00\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4523 - accuracy: 0.0000e+00 - val_loss: 1.1920 - val_accuracy: 0.0000e+00\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 10ms/step - loss: 0.4496 - accuracy: 0.0000e+00 - val_loss: 1.2682 - val_accuracy: 0.0000e+00\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 9ms/step - loss: 0.4507 - accuracy: 0.0000e+00 - val_loss: 1.2028 - val_accuracy: 0.0000e+00\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4503 - accuracy: 0.0000e+00 - val_loss: 1.2455 - val_accuracy: 0.0000e+00\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4500 - accuracy: 0.0000e+00 - val_loss: 1.2148 - val_accuracy: 0.0000e+00\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4557 - accuracy: 0.0000e+00 - val_loss: 1.1857 - val_accuracy: 0.0000e+00\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4593 - accuracy: 0.0000e+00 - val_loss: 1.2403 - val_accuracy: 0.0000e+00\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4567 - accuracy: 0.0000e+00 - val_loss: 1.2523 - val_accuracy: 0.0000e+00\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4455 - accuracy: 0.0000e+00 - val_loss: 1.2431 - val_accuracy: 0.0000e+00\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4183 - accuracy: 0.0000e+00 - val_loss: 1.2153 - val_accuracy: 0.0000e+00\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4333 - accuracy: 0.0000e+00 - val_loss: 1.1882 - val_accuracy: 0.0000e+00\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4689 - accuracy: 0.0000e+00 - val_loss: 1.2124 - val_accuracy: 0.0000e+00\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4629 - accuracy: 0.0000e+00 - val_loss: 1.1965 - val_accuracy: 0.0000e+00\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4217 - accuracy: 0.0000e+00 - val_loss: 1.1906 - val_accuracy: 0.0000e+00\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4363 - accuracy: 0.0000e+00 - val_loss: 1.2116 - val_accuracy: 0.0000e+00\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4295 - accuracy: 0.0000e+00 - val_loss: 1.2086 - val_accuracy: 0.0000e+00\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4169 - accuracy: 0.0000e+00 - val_loss: 1.2564 - val_accuracy: 0.0000e+00\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4770 - accuracy: 0.0000e+00 - val_loss: 1.2847 - val_accuracy: 0.0000e+00\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4965 - accuracy: 0.0000e+00 - val_loss: 1.1869 - val_accuracy: 0.0000e+00\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4410 - accuracy: 0.0000e+00 - val_loss: 1.2141 - val_accuracy: 0.0000e+00\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4157 - accuracy: 0.0000e+00 - val_loss: 1.2526 - val_accuracy: 0.0000e+00\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.3977 - accuracy: 0.0000e+00 - val_loss: 1.1965 - val_accuracy: 0.0000e+00\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 8ms/step - loss: 0.4193 - accuracy: 0.0000e+00 - val_loss: 1.2198 - val_accuracy: 0.0000e+00\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4364 - accuracy: 0.0000e+00 - val_loss: 1.2922 - val_accuracy: 0.0000e+00\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4421 - accuracy: 0.0000e+00 - val_loss: 1.2099 - val_accuracy: 0.0000e+00\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.4307 - accuracy: 0.0000e+00 - val_loss: 1.2456 - val_accuracy: 0.0000e+00\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4534 - accuracy: 0.0000e+00 - val_loss: 1.3067 - val_accuracy: 0.0000e+00\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4250 - accuracy: 0.0000e+00 - val_loss: 1.1798 - val_accuracy: 0.0000e+00\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4446 - accuracy: 0.0000e+00 - val_loss: 1.2437 - val_accuracy: 0.0000e+00\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.4689 - accuracy: 0.0000e+00 - val_loss: 1.2548 - val_accuracy: 0.0000e+00\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4272 - accuracy: 0.0000e+00 - val_loss: 1.2448 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f25e4e3af70>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 5 Colab Notebook to your Github repository under \"Day 5\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}