# Optimization

## Topics covered in today's module
* Optimization
* Gradident Descent
* Optimizers(SGD, ADAM, etc.)

## Main takeaways from doing today's assignment
- Different types of gradient descent like mini-batch, stochastic and batch gradient descent
- How to use them in keras

## Challenging, interesting, or exciting aspects of today's assignment
Relation between optimizers and loss functions

## Additional resources used 
medium, stackoverflow, geeksforgeeks
